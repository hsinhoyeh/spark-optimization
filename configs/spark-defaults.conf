# Core settings
spark.master=spark://spark-master:7077
spark.driver.memory=8g
spark.executor.memory=24g
spark.executor.cores=8
spark.executor.instances=2
spark.executor.memoryOverhead=2g
spark.driver.maxResultSize=4g

# Storage settings - using both NVMes for local storage
spark.local.dir=/spark/nvme1/tmp,/spark/nvme2/tmp

# I/O optimization
spark.io.compression.codec=lz4
spark.io.compression.lz4.blockSize=512k
spark.file.transferTo=true

# Memory tuning
spark.memory.fraction=0.8
spark.memory.storageFraction=0.3

# Shuffle optimization
spark.shuffle.file.buffer=64k
spark.shuffle.compress=true
spark.shuffle.spill.compress=true
spark.shuffle.service.enabled=true
spark.shuffle.sort.bypassMergeThreshold=10000

# Disk-related tuning
spark.disk.spillCompress=true
spark.rdd.compress=true
spark.serializer=org.apache.spark.serializer.KryoSerializer
spark.kryoserializer.buffer.max=1g
spark.kryoserializer.buffer=64m

# Data locality preference - critical for NVMe performance
spark.locality.wait=100ms
spark.locality.wait.node=300ms
spark.locality.wait.process=1s

# Network settings
spark.reducer.maxSizeInFlight=96m
spark.maxRemoteBlockSizeFetchToMem=200m
spark.network.timeout=600s

# SQL optimization
spark.sql.files.maxPartitionBytes=134217728
spark.sql.files.openCostInBytes=4194304
spark.sql.shuffle.partitions=200
spark.sql.adaptive.enabled=true
spark.sql.adaptive.coalescePartitions.enabled=true
spark.sql.adaptive.skewJoin.enabled=true

# JVM and GC tuning
spark.executor.extraJavaOptions=-XX:+UseG1GC -XX:+UnlockDiagnosticVMOptions -XX:+G1SummarizeConcMark -XX:InitiatingHeapOccupancyPercent=35 -XX:ConcGCThreads=12 -XX:+PrintGCDetails -XX:+PrintGCDateStamps -Xloggc:/var/log/gc.log
spark.driver.extraJavaOptions=-XX:+UseG1GC -XX:+UnlockDiagnosticVMOptions -XX:+G1SummarizeConcMark -XX:InitiatingHeapOccupancyPercent=35 -XX:ConcGCThreads=12

# Storage format options
spark.sql.parquet.compression.codec=snappy
spark.sql.parquet.mergeSchema=false
spark.sql.parquet.filterPushdown=true
spark.sql.hive.metastorePartitionPruning=true

# Storage level for frequently used RDDs
spark.storage.level=MEMORY_AND_DISK_SER

# Dynamic allocation
spark.dynamicAllocation.enabled=true
spark.dynamicAllocation.initialExecutors=2
spark.dynamicAllocation.minExecutors=2
spark.dynamicAllocation.maxExecutors=10
spark.dynamicAllocation.executorIdleTimeout=60s
